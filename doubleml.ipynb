{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "All the imports. simutils contains a set of functions I created for generating synthetic data for these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels\n",
    "#!pip install scikit-learn\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install scipy\n",
    "\n",
    "import simutils as sim\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Variables of Non-Interest\n",
    "First, create some synthetic age, credit score and arbitrary hypothetically relevant personality characteristic values. Most of these variables are nuisance or confounding variables, but will partially determine both the predictor of interest and the dependent variable.\n",
    "\n",
    "Age will be normally distributed (M=35, SD=10) and range from 18 to 78. Credit scores are 700-730 on average, but tend to be lower for younger people, and are reduced by up to 40 points for the youngest, relative to the oldest. The hypothetical personality variable will reflect whatever set of intrinsic characteristics (e.g., inquisitiveness, intelligence) that are normally distributed and might drive the behaviour of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "split=int(N/2)\n",
    "ages=sim.generate_ages(35, 10, N)\n",
    "credit_scores=sim.generate_credit_scores(ages)\n",
    "personality=np.random.normal(loc=0, scale=1, size=N)\n",
    "#create data frame out of factors\n",
    "Z=pd.DataFrame({\n",
    "    'age': ages,\n",
    "    'credit_score': credit_scores,\n",
    "    'personality': personality\n",
    "    })\n",
    "train_Z=Z.iloc[:split,:]\n",
    "test_Z=Z.iloc[split:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor of Interest\n",
    "The independent measure will be the number of recorded interactions with product information pages, obtained from cookies, since monitoring was implemented.\n",
    "\n",
    "Our goal will be to determine the causal relationship between clicking behavior and the dependent variable, personal wealth. This relationship can be quantified by computing the coefficient or weight in a regression or machine learning model that predicts wealth from clicks.\n",
    "\n",
    "Clicking behavior will be influenced by all three of the variables in Z. These influences will be non-linear, which would make a linear regression-based approach a poor choice, though the number of clicks will be a linear combination of all three influences. I will explicitly determine each confound's influence on an individual's clicking behavior, making it easier to establish the ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_delta</th>\n",
       "      <th>csv_delta</th>\n",
       "      <th>personality_delta</th>\n",
       "      <th>baseline</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>-16</td>\n",
       "      <td>22</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>-2</td>\n",
       "      <td>41</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_delta  csv_delta  personality_delta  baseline  total\n",
       "0         43          6                 16        15     80\n",
       "1         37         34                 11        29    111\n",
       "2         70         11                -16        22     87\n",
       "3         63         16                 -2        41    118\n",
       "4         -3         45                 12        43     97"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "clicks=sim.generate_clicks(Z)\n",
    "clicks.columns=['age_delta', 'csv_delta', 'personality_delta']\n",
    "#external random factors are responsible for the baseline number of interactions\n",
    "clicks['baseline'] = randint.rvs(10,50, size=N)\n",
    "clicks['total'] = clicks.apply(np.sum, axis=1)\n",
    "train_clicks=clicks.iloc[:split,:]\n",
    "test_clicks=clicks.iloc[split,:]\n",
    "train_clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the perspective of an all-knowing oracle, we can isolate the total number of clicks for each person that were not attributable to either age or credit score value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_clicks=pd.DataFrame(clicks['personality_delta']+clicks['baseline'])\n",
    "clean_clicks.columns=['i_total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dependent Variable: Wealth/Value\n",
    "Now to create the dependent variable, portfolio value, we can use a similar procedure. The DV value will be caused in part by the number of clicks, as well as age and credit score and we can see whether we can tease apart these components. **Every click causes an increases a person's wealth by $500**, and this is the causal parameter we are trying to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_wealth</th>\n",
       "      <th>csv_wealth</th>\n",
       "      <th>click_wealth</th>\n",
       "      <th>circumstance_wealth</th>\n",
       "      <th>total_wealth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18379</td>\n",
       "      <td>54368</td>\n",
       "      <td>40008</td>\n",
       "      <td>40969</td>\n",
       "      <td>153724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15229</td>\n",
       "      <td>39603</td>\n",
       "      <td>55490</td>\n",
       "      <td>36873</td>\n",
       "      <td>147195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35530</td>\n",
       "      <td>60454</td>\n",
       "      <td>43501</td>\n",
       "      <td>37761</td>\n",
       "      <td>177246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38973</td>\n",
       "      <td>63080</td>\n",
       "      <td>59001</td>\n",
       "      <td>42377</td>\n",
       "      <td>203431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11037</td>\n",
       "      <td>32217</td>\n",
       "      <td>48501</td>\n",
       "      <td>29395</td>\n",
       "      <td>121150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age_wealth  csv_wealth  click_wealth  circumstance_wealth  total_wealth\n",
       "0       18379       54368         40008                40969        153724\n",
       "1       15229       39603         55490                36873        147195\n",
       "2       35530       60454         43501                37761        177246\n",
       "3       38973       63080         59001                42377        203431\n",
       "4       11037       32217         48501                29395        121150"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline wealth increases as individuals get older until ~75, at which point it drops off\n",
    "age_wealth=Z.apply(lambda row: sim.age_worth(row[0]), axis=1)\n",
    "#baseline wealth increases for people with better credit scores, who can get better rates\n",
    "csv_wealth=Z.apply(lambda row: sim.csv_worth(row[1]), axis=1)\n",
    "#wealth increases caused by the behavior we care about (clicks), itself influenced by age and credit\n",
    "click_wealth=clicks.apply(lambda row: sim.clicks_worth(row[4]), axis=1)\n",
    "#wealth attributable to all the other random chance factors\n",
    "circumstance_wealth=clicks.apply(lambda row: sim.circumstance_worth(), axis=1)\n",
    "wealth=pd.concat([age_wealth, csv_wealth, click_wealth, circumstance_wealth], axis=1)\n",
    "wealth.columns=['age_wealth', 'csv_wealth', 'click_wealth', 'circumstance_wealth']\n",
    "wealth[\"total_wealth\"]=wealth.apply(np.sum, axis=1)\n",
    "train_wealth=wealth.iloc[:split,:]\n",
    "test_wealth=wealth.iloc[split:,:]\n",
    "train_wealth.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for clicks, I want the oracle measure of the wealth attributable to those clicks that were not driven by age and credit score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_wealth=clean_clicks.apply(lambda row: sim.clicks_worth(row[0]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Multiple Regression\n",
    "So the goal is to use the customer's interaction data (\"clicks\") to explain the value of their portfolio (\"total wealth\"). In this scenario, we have reason to believe that age and credit score affect both the number of interactions and the customer's total wealth and need to remove these confounding effects. Before we do so, let's run a naive multiple regression, paying attention to the coefficient for the clicks variable, to see the impact of the confounding variables on our parameter estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.904\n",
      "Model:                            OLS   Adj. R-squared:                  0.904\n",
      "Method:                 Least Squares   F-statistic:                 3.154e+04\n",
      "Date:                Thu, 16 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        13:20:14   Log-Likelihood:            -1.0645e+05\n",
      "No. Observations:               10000   AIC:                         2.129e+05\n",
      "Df Residuals:                    9996   BIC:                         2.129e+05\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.132e+05   6111.158     18.530      0.000    1.01e+05    1.25e+05\n",
      "clicks       658.4315      4.955    132.881      0.000     648.719     668.144\n",
      "age         3915.9468     91.686     42.710      0.000    3736.224    4095.670\n",
      "csv         -214.3509     13.227    -16.205      0.000    -240.279    -188.423\n",
      "==============================================================================\n",
      "Omnibus:                     6668.937   Durbin-Watson:                   2.006\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           207750.295\n",
      "Skew:                           2.751   Prob(JB):                         0.00\n",
      "Kurtosis:                      24.641   Cond. No.                     4.28e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.28e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y=wealth[\"total_wealth\"]\n",
    "x=clicks[\"total\"]\n",
    "basedf=pd.DataFrame({'y': y, 'clicks': x})\n",
    "basedf[\"age\"]=Z[\"age\"]\n",
    "basedf[\"csv\"]=Z[\"credit_score\"]\n",
    "\n",
    "naive_model = smf.ols(formula='y ~ 1 + clicks + age + csv', data = basedf).fit()\n",
    "print(naive_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model summary, the coefficient estimate for clicks is quite biased: we know the true value to be 500, so the model is greatly overestimating the impact of clicking. There's also an indicator of potential multicollinearity, which is not at all surprising, since age drives csv, and age and csv drive the number of clicks.\n",
    "\n",
    "## Double Machine Learning\n",
    "Now let's use double machine learning to try to remove the confounding effects of age and credit score. This is done by using the confounders to predict the variables we care about (clicks and wealth), and calculating the residuals. The residualized values are the components of the variables of interest that couldn't be adequately predicted by the confounders, and are orthogonal with respect to the confounding variables.\n",
    "\n",
    "The particular ML algorithm used isn't critical, as long as it is appropriate for your variables. Here, I am predicting continuous integer values, and had never before had an opportunity to try out a boosted decision tree algorithm. For categorical data, an SVM or even neural network might be appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Y_hat   R-squared:                       0.711\n",
      "Model:                            OLS   Adj. R-squared:                  0.711\n",
      "Method:                 Least Squares   F-statistic:                 1.229e+04\n",
      "Date:                Thu, 16 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        13:20:15   Log-Likelihood:                -49646.\n",
      "No. Observations:                5000   AIC:                         9.930e+04\n",
      "Df Residuals:                    4998   BIC:                         9.931e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -84.5802     70.231     -1.204      0.229    -222.264      53.103\n",
      "X_hat        498.1929      4.494    110.867      0.000     489.384     507.002\n",
      "==============================================================================\n",
      "Omnibus:                        1.908   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.385   Jarque-Bera (JB):                1.889\n",
      "Skew:                           0.024   Prob(JB):                        0.389\n",
      "Kurtosis:                       3.083   Cond. No.                         15.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "M_clicks=GradientBoostingRegressor()\n",
    "M_wealth=GradientBoostingRegressor()\n",
    "\n",
    "#use cross-validation values to obtain residuals\n",
    "residualized_y = y-cross_val_predict(M_wealth, Z[[\"age\", \"credit_score\"]], y, cv=3)\n",
    "residualized_x = x-cross_val_predict(M_clicks, Z[[\"age\", \"credit_score\"]], x, cv=3)\n",
    "df=pd.DataFrame()\n",
    "df[\"Y_hat\"]=residualized_y\n",
    "df[\"X_hat\"]=residualized_x\n",
    "df[\"clean_X\"]=clean_clicks\n",
    "df[\"dirty_Y\"]=wealth[\"click_wealth\"]\n",
    "df[\"clean_Y\"]=clean_wealth\n",
    "df[\"age\"]=Z[\"age\"]\n",
    "df[\"credit_score\"]=Z[\"credit_score\"]\n",
    "\n",
    "train_df=df.iloc[:split,:]\n",
    "test_df=df.iloc[split:,:]\n",
    "\n",
    "#fit the residualized data and check out the model prediction\n",
    "DML_model = smf.ols(formula='Y_hat ~ 1 + X_hat', data = train_df).fit()\n",
    "print(DML_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Models\n",
    "The double-ML model using the orthogonalized values are much closer to the true parameter value.\n",
    "\n",
    "From the perspective of an all-knowing oracle, we can compare the double-ML results against models using the ground-truth oracle data. In my first attempt, I modeled wealth values that were computed only using the total number of clicks (which itself was influenced by age and credit score), which I then realized doesn't quite capture what double-ML was supposed to have done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                dirty_Y   R-squared:                       0.475\n",
      "Model:                            OLS   Adj. R-squared:                  0.475\n",
      "Method:                 Least Squares   F-statistic:                     4521.\n",
      "Date:                Thu, 16 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        13:20:44   Log-Likelihood:                -51874.\n",
      "No. Observations:                5000   AIC:                         1.038e+05\n",
      "Df Residuals:                    4998   BIC:                         1.038e+05\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3.207e+04    307.405    104.326      0.000    3.15e+04    3.27e+04\n",
      "clean_X      497.0046      7.392     67.238      0.000     482.514     511.496\n",
      "==============================================================================\n",
      "Omnibus:                      118.645   Durbin-Watson:                   2.022\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               87.068\n",
      "Skew:                           0.225   Prob(JB):                     1.24e-19\n",
      "Kurtosis:                       2.536   Cond. No.                         117.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#fit the number of clicks unrelated to age/credit score to the wealth attributable to clicks alone\n",
    "Oracle_model_1 = smf.ols(formula='dirty_Y ~ 1 + clean_X', data = train_df).fit()\n",
    "print(Oracle_model_1.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed up by repeating the model, using the click behavior not attributable to age and credit score to generate the wealth attributable to the clicking behavior that is also not attributable to age and credit score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                clean_Y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 8.394e+09\n",
      "Date:                Thu, 16 Nov 2023   Prob (F-statistic):               0.00\n",
      "Time:                        13:21:06   Log-Likelihood:                -15818.\n",
      "No. Observations:                5000   AIC:                         3.164e+04\n",
      "Df Residuals:                    4998   BIC:                         3.165e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.9067      0.227     -3.995      0.000      -1.352      -0.462\n",
      "clean_X      500.0116      0.005   9.16e+04      0.000     500.001     500.022\n",
      "==============================================================================\n",
      "Omnibus:                     3900.055   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              295.408\n",
      "Skew:                          -0.022   Prob(JB):                     7.13e-65\n",
      "Kurtosis:                       1.810   Cond. No.                         117.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#fit the number of clicks unrelated to age/credit score to the wealth attributable to clicks alone\n",
    "Oracle_model_2 = smf.ols(formula='clean_Y ~ 1 + clean_X', data = train_df).fit()\n",
    "print(Oracle_model_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Orthogonalized Scores to Oracle Values\n",
    "A final question to consider is whether the orthogonalized scores are equivalent to \"clean\" data; that is, whether they map to the ground-truth oracle data without the influence of the nuisance variables. Consider that we have access to the components of each person's click behaviour that are independent of age and credit score. We can quantify the correspondence of these values with the residualized scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The residualized values are correlated at 94% with the true values and 71% with the dirty values\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res=stats.spearmanr(residualized_x, clean_clicks)\n",
    "bad=stats.spearmanr(residualized_x, clicks[\"total\"])\n",
    "print(f\"The residualized values are correlated at {int(res.statistic*100)}% with the true values and {int(bad.statistic *100)}% with the dirty values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Average Treatment Effects (CATE)\n",
    "Trying to get my head wrapped around how CATE is implemented. In all the examples I have seen, after the residuals are calculated, CATE is computed by predicting Y_hat using the treatment * nuisance variables. This strikes me as bizarre, since the nuisance variables are theoretically unrelated to the residualized values, and thus all regression coefficients should be zero (they won't be exactly zero, but that's because of error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_hat</th>\n",
       "      <th>X_hat</th>\n",
       "      <th>clean_X</th>\n",
       "      <th>dirty_Y</th>\n",
       "      <th>clean_Y</th>\n",
       "      <th>age</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1340.155037</td>\n",
       "      <td>9.800987</td>\n",
       "      <td>42</td>\n",
       "      <td>53002</td>\n",
       "      <td>20999</td>\n",
       "      <td>32</td>\n",
       "      <td>683</td>\n",
       "      <td>497.380323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>-6430.844963</td>\n",
       "      <td>-20.199013</td>\n",
       "      <td>20</td>\n",
       "      <td>37998</td>\n",
       "      <td>9998</td>\n",
       "      <td>32</td>\n",
       "      <td>681</td>\n",
       "      <td>496.237540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>-12169.927278</td>\n",
       "      <td>-24.587529</td>\n",
       "      <td>14</td>\n",
       "      <td>39995</td>\n",
       "      <td>6994</td>\n",
       "      <td>28</td>\n",
       "      <td>654</td>\n",
       "      <td>494.702142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>-5097.596630</td>\n",
       "      <td>-6.602138</td>\n",
       "      <td>33</td>\n",
       "      <td>36007</td>\n",
       "      <td>16501</td>\n",
       "      <td>39</td>\n",
       "      <td>729</td>\n",
       "      <td>499.353031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>-4371.321940</td>\n",
       "      <td>-18.287082</td>\n",
       "      <td>24</td>\n",
       "      <td>34497</td>\n",
       "      <td>11991</td>\n",
       "      <td>37</td>\n",
       "      <td>716</td>\n",
       "      <td>498.871028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y_hat      X_hat  clean_X  dirty_Y  clean_Y  age  credit_score  \\\n",
       "5000   1340.155037   9.800987       42    53002    20999   32           683   \n",
       "5001  -6430.844963 -20.199013       20    37998     9998   32           681   \n",
       "5002 -12169.927278 -24.587529       14    39995     6994   28           654   \n",
       "5003  -5097.596630  -6.602138       33    36007    16501   39           729   \n",
       "5004  -4371.321940 -18.287082       24    34497    11991   37           716   \n",
       "\n",
       "            cate  \n",
       "5000  497.380323  \n",
       "5001  496.237540  \n",
       "5002  494.702142  \n",
       "5003  499.353031  \n",
       "5004  498.871028  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#fit the residualized data and check out the model prediction\n",
    "cate_model = smf.ols(formula='Y_hat ~ X_hat * (age + credit_score)', data = train_df).fit()\n",
    "\n",
    "cate_test = test_df.assign(cate=cate_model.predict(test_df.assign(X_hat=1))\n",
    "                        - cate_model.predict(test_df.assign(X_hat=0)))\n",
    "cate_test.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
